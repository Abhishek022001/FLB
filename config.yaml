model:
  model_path: "microsoft/Phi-3-mini-4k-instruct"
  quantization: 4 # 4 or 8 or null. Set 4 or 8 to use Qlora. Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision.
  device_map: 'cuda' # support cuda, cpu, mps and mlx for os system
  torch_dtype: 'bfloat16'
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"] # finetune target layers, only `torch.nn.Linear` and `Conv1D` are supported
  lora:
    peft_lora_r: 4 # It determineås the rank of the low-rank matrix and affects the representation ability of the model.
    peft_lora_alpha: 8 # It is a scaling factor that adjusts the contribution of the low-rank matrix to the overall model.
sft:
  learning_rate_max: 5e-5
  learning_rate_min: 1e-6
  max_seq_length: 2048 # The maximum sequence length to use for the `ConstantLengthDataset` and for automaticallty creating the Dataset
  clip_threshold: 10 # Limit the maximum amplitude of the data before performing sensitivity calculations or adding noise
  dp_fedavg_gaussian_enabled: True # use gaussian noise after dp clipping in client side
  epsilon: 1 # Used to quantify the strength of privacy protection. The smaller ε is, the stronger the privacy protection is. In the context of differential privacy, ε controls the uncertainty in the algorithm output caused by adding noise.
  sensitivity: 1 # The maximum impact of a single piece of data on the query or analysis results
  delta: 1e-5 # The upper limit of the probability that the system allows privacy protection to fail is given
  training_arguments:
    output_dir: "./output" # to be set by hydra
    overwrite_output_dir: True
    remove_unused_columns: True
    seed: 1234
    learning_rate: 5e-6 # to be set by the client
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 1
    logging_steps: 20
    log_level: "info"
    logging_strategy: "steps"
    num_train_epochs: 1
    max_steps: 10
    save_steps: 100
    save_total_limit: 1
    gradient_checkpointing: True
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.2
    do_eval: False
mlx:
    lora_layers: 16 # Number of layers to fine-tune by using mlx
    learning_rate: 1e-5 # Adam learning rate.
    resume_adapter_file: null # Load path to resume training with the given adapter weights.
    adapter_path: "adapters" # Save/load path for the trained adapter weights.
    train: True
    test: false # Evaluate on the test set after training
    test_batches: 100 # Number of test set batches, -1 uses the entire test set.
    use_dora: false # Use DoRA instead of LoRA.
    lr_schedule: null
    seed: 1212
    train_arg:
      batch_size: 4 # Minibatch size.
      iters: 100 # Iterations to train for.
      val_batches: 25 # Number of validation batches, -1 uses the entire validation set.
      steps_per_report: 10 # Number of training steps between loss reporting.
      steps_per_eval: 50 # Number of training steps between validations.
      save_every: 50 # Save the model every N iterations.
      max_seq_length: 2048 # Maximum sequence length.
      grad_checkpoint: false # Use gradient checkpointing to reduce memory use.
client:
  host: 127.0.0.1
  port: 8088
  local_dp: False
  grpc_insecure: True # you can turn off this and set grpc_auth_cer_path to use secure gRPC channel
  grpc_auth_cer_path: null # set your local root certificates path to here
  weight_file_download_path: "./client/weights_update" # the path to save weight file from server side
  auto_pull: True # set it to False if you want to copy weight file from server side manually. If it's False, make sure you already put the weight file to the right place before you call update function in client side.
server:
  host: 127.0.0.1
  port: 8088
  clip_threshold: 2 # dp fixed clipping threshold in server side
  noise_multiplier: 1 # A larger noise_multiplier means more noise, thus stronger privacy protection, but may also lead to a decrease in model performance. A smaller noise_multiplier may provide better model performance but weaker privacy protection.
  restful_url: "http://127.0.0.1:8080"
  clients_file_save_path: "./save" # the path of the weight file sent by the clients
  output_path: "./server_output" # the save path of the model file after weight aggregation and evaluation result file
dataset_name: "./examples/datasets/datasets" # Specify a custom dataset or huggingface dataset, for example: medalpaca/medical_meadow_medical_flashcards
num_clients: 1 # client number in federated learning
chain_record: False # whether to connect to chain or not